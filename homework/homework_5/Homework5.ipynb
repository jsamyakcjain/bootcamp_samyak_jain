{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f416e6af-8bbe-478d-b4ad-f4b80aab1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, Literal, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601fa019-e5b1-44a5-94d0-0130f553a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_sample_transaction_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a sample DataFrame for commercial credit card transactions.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'TransactionID': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "        'CardholderName': ['John Doe', 'Jane Smith', 'Peter Jones', 'John Doe', 'Jane Smith', 'Sarah Lee'],\n",
    "        'CardNumber': ['XXXX-1234', 'XXXX-5678', 'XXXX-9012', 'XXXX-1234', 'XXXX-5678', 'XXXX-3456'],\n",
    "        'TransactionDate': pd.to_datetime([\n",
    "            '2025-08-15 09:30:00', '2025-08-16 14:15:00', '2025-08-17 11:00:00',\n",
    "            '2025-08-18 18:45:00', '2025-08-19 08:00:00', '2025-08-20 10:20:00'\n",
    "        ]),\n",
    "        'MerchantName': ['Office Depot', 'United Airlines', 'Hilton Garden Inn', 'Uber Eats', 'Staples', 'Amazon Web Services'],\n",
    "        'MerchantCategoryCode': [5044, 4511, 7011, 5812, 5111, 4814],\n",
    "        'TransactionAmount': [152.75, 850.50, 210.00, 45.20, 89.99, 500.00],\n",
    "        'Currency': ['USD', 'USD', 'USD', 'USD', 'USD', 'USD'],\n",
    "        'GLCode': [6050, 5010, 5020, 6060, 6050, 6080],\n",
    "        'ProjectCode': ['P-101', 'P-102', 'P-102', 'P-101', 'P-103', 'P-104'],\n",
    "        'IsApproved': [True, True, True, True, False, True]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Ensure this script is in the same directory as your .env file, which should contain:\n",
    "# DATA_DIR_RAW=data/raw\n",
    "# DATA_DIR_PROCESSED=data/processed\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb7ffe3-378b-4459-8894-e6202dee61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directories...\n",
      "Directories created/verified.\n",
      "\n",
      "Generating sample commercial credit card transaction data...\n",
      "Sample DataFrame created successfully.\n",
      "   TransactionID CardholderName CardNumber     TransactionDate  \\\n",
      "0           1001       John Doe  XXXX-1234 2025-08-15 09:30:00   \n",
      "1           1002     Jane Smith  XXXX-5678 2025-08-16 14:15:00   \n",
      "2           1003    Peter Jones  XXXX-9012 2025-08-17 11:00:00   \n",
      "3           1004       John Doe  XXXX-1234 2025-08-18 18:45:00   \n",
      "4           1005     Jane Smith  XXXX-5678 2025-08-19 08:00:00   \n",
      "\n",
      "        MerchantName  MerchantCategoryCode  TransactionAmount Currency  \\\n",
      "0       Office Depot                  5044             152.75      USD   \n",
      "1    United Airlines                  4511             850.50      USD   \n",
      "2  Hilton Garden Inn                  7011             210.00      USD   \n",
      "3          Uber Eats                  5812              45.20      USD   \n",
      "4            Staples                  5111              89.99      USD   \n",
      "\n",
      "   GLCode ProjectCode  IsApproved  \n",
      "0    6050       P-101        True  \n",
      "1    5010       P-102        True  \n",
      "2    5020       P-102        True  \n",
      "3    6060       P-101        True  \n",
      "4    6050       P-103       False  \n",
      "\n",
      "Saving DataFrame to CSV at: data/raw/commercial_credit_card_transactions.csv\n",
      "CSV file saved successfully!\n",
      "Saving DataFrame to Parquet at: data/processed/commercial_credit_card_transactions.parquet\n",
      "Parquet file saved successfully!\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define directory paths using environment variables with defaults\n",
    "    DATA_DIR_RAW = pathlib.Path(os.getenv(\"DATA_DIR_RAW\", \"data/raw\"))\n",
    "    DATA_DIR_PROCESSED = pathlib.Path(os.getenv(\"DATA_DIR_PROCESSED\", \"data/processed\"))\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    print(\"Creating directories...\")\n",
    "    DATA_DIR_RAW.mkdir(parents=True, exist_ok=True)\n",
    "    DATA_DIR_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Directories created/verified.\")\n",
    "\n",
    "    # Create the sample commercial credit card transaction data\n",
    "    print(\"\\nGenerating sample commercial credit card transaction data...\")\n",
    "    df = create_sample_transaction_data()\n",
    "    print(\"Sample DataFrame created successfully.\")\n",
    "    print(df.head())\n",
    "\n",
    "    # --- Save to CSV ---\n",
    "    csv_file_path = DATA_DIR_RAW / \"commercial_credit_card_transactions.csv\"\n",
    "    print(f\"\\nSaving DataFrame to CSV at: {csv_file_path}\")\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(\"CSV file saved successfully!\")\n",
    "\n",
    "    # --- Save to Parquet ---\n",
    "    parquet_file_path = DATA_DIR_PROCESSED / \"commercial_credit_card_transactions.parquet\"\n",
    "    print(f\"Saving DataFrame to Parquet at: {parquet_file_path}\")\n",
    "    df.to_parquet(parquet_file_path)\n",
    "    print(\"Parquet file saved successfully!\")\n",
    "\n",
    "    print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a2e013-d2b7-4809-9c1c-cfdea6c46b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation on the DataFrame...\n",
      "\n",
      "Shape Check: ✅ PASSED. Shape is (6, 5).\n",
      "Dtype Check: ✅ PASSED. All critical column dtypes are correct.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, Any\n",
    "\n",
    "def validate_dataframe(df: pd.DataFrame, expected_shape: tuple, expected_dtypes: Dict[str, Any]) -> dict:\n",
    "    \"\"\"\n",
    "    Performs validation checks on a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    \n",
    "    # Check shape\n",
    "    if df.shape == expected_shape:\n",
    "        validation_results['Shape Check'] = f\"✅ PASSED. Shape is {df.shape}.\"\n",
    "    else:\n",
    "        validation_results['Shape Check'] = f\"❌ FAILED. Expected {expected_shape}, but got {df.shape}.\"\n",
    "        \n",
    "    # Check critical column dtypes\n",
    "    dtype_mismatches = []\n",
    "    for col, expected_dtype in expected_dtypes.items():\n",
    "        if col not in df.columns:\n",
    "            dtype_mismatches.append(f\"Column '{col}' is missing.\")\n",
    "        elif str(df[col].dtype) != str(expected_dtype):\n",
    "            dtype_mismatches.append(f\"Column '{col}': Expected '{expected_dtype}', but got '{df[col].dtype}'.\")\n",
    "            \n",
    "    if not dtype_mismatches:\n",
    "        validation_results['Dtype Check'] = \"✅ PASSED. All critical column dtypes are correct.\"\n",
    "    else:\n",
    "        validation_results['Dtype Check'] = \"❌ FAILED. Dtype mismatches found:\\n- \" + \"\\n- \".join(dtype_mismatches)\n",
    "        \n",
    "    return validation_results\n",
    "\n",
    "# --- The code below is what you need to add ---\n",
    "\n",
    "# Create the sample commercial credit card transaction data\n",
    "data = {\n",
    "    'TransactionID': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "    'CardholderName': ['John Doe', 'Jane Smith', 'Peter Jones', 'John Doe', 'Jane Smith', 'Sarah Lee'],\n",
    "    'TransactionDate': pd.to_datetime(['2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20']),\n",
    "    'TransactionAmount': [152.75, 850.50, 210.00, 45.20, 89.99, 500.00],\n",
    "    'IsApproved': [True, True, True, True, False, True]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Define expected values for validation ---\n",
    "expected_rows = 6\n",
    "expected_cols = 5\n",
    "expected_shape = (expected_rows, expected_cols)\n",
    "expected_dtypes = {\n",
    "    'TransactionID': 'int64',\n",
    "    'TransactionDate': 'datetime64[ns]',\n",
    "    'TransactionAmount': 'float64',\n",
    "    'IsApproved': 'bool'\n",
    "}\n",
    "\n",
    "# --- Run the validation by calling the function and assigning the result ---\n",
    "print(\"Running validation on the DataFrame...\\n\")\n",
    "validation_report = validate_dataframe(df, expected_shape, expected_dtypes)\n",
    "\n",
    "# --- Display the results by printing the returned dictionary ---\n",
    "for check, result in validation_report.items():\n",
    "    print(f\"{check}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db30cbba-4a7e-4fca-a2fb-59595a44014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Writing DataFrames ---\n",
      "✅ DataFrame successfully saved to CSV at: data/raw/sample_data.csv\n",
      "✅ DataFrame successfully saved to Parquet at: data/processed/sample_data.parquet\n",
      "\n",
      "--- Reading DataFrames ---\n",
      "✅ DataFrame successfully read from CSV: data/raw/sample_data.csv\n",
      "\n",
      "DataFrame read from CSV head:\n",
      "   id value\n",
      "0   1     a\n",
      "1   2     b\n",
      "2   3     c\n",
      "✅ DataFrame successfully read from Parquet: data/processed/sample_data.parquet\n",
      "\n",
      "DataFrame read from Parquet head:\n",
      "   id value\n",
      "0   1     a\n",
      "1   2     b\n",
      "2   3     c\n",
      "\n",
      "--- Demonstrating File Not Found Error ---\n",
      "❌ Error: File not found at data/raw/non_existent_file.csv\n"
     ]
    }
   ],
   "source": [
    "def write_df(df: pd.DataFrame, file_path: pathlib.Path, file_type: Literal['csv', 'parquet']):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a specified file path based on file type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        file_path (pathlib.Path): The full path to the file, including the filename.\n",
    "        file_type (Literal['csv', 'parquet']): The format to save in.\n",
    "    \"\"\"\n",
    "    # Create the parent directory if it doesn't exist\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"✅ DataFrame successfully saved to CSV at: {file_path}\")\n",
    "        elif file_type == 'parquet':\n",
    "            df.to_parquet(file_path, index=False)\n",
    "            print(f\"✅ DataFrame successfully saved to Parquet at: {file_path}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Use 'csv' or 'parquet'.\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ Error: Required library for '{file_type}' is not installed.\")\n",
    "        if file_type == 'parquet':\n",
    "            print(\"Please install pyarrow or fastparquet: `pip install pyarrow` or `pip install fastparquet`.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred while writing the DataFrame: {e}\")\n",
    "        raise\n",
    "\n",
    "def read_df(file_path: pathlib.Path) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Reads a DataFrame from a file based on its suffix (.csv or .parquet).\n",
    "\n",
    "    Args:\n",
    "        file_path (pathlib.Path): The full path to the file to read.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "        None: If the file does not exist or an error occurs.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"❌ Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"✅ DataFrame successfully read from CSV: {file_path}\")\n",
    "        elif file_path.suffix == '.parquet':\n",
    "            df = pd.read_parquet(file_path)\n",
    "            print(f\"✅ DataFrame successfully read from Parquet: {file_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Error: Unsupported file suffix '{file_path.suffix}'. Only .csv and .parquet are supported.\")\n",
    "            return None\n",
    "        return df\n",
    "    except ImportError:\n",
    "        print(f\"❌ Error: Required library for '{file_path.suffix}' is not installed.\")\n",
    "        if file_path.suffix == '.parquet':\n",
    "            print(\"Please install pyarrow or fastparquet: `pip install pyarrow` or `pip install fastparquet`.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred while reading the DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample DataFrame for demonstration\n",
    "    data = {'id': [1, 2, 3], 'value': ['a', 'b', 'c']}\n",
    "    sample_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"--- Writing DataFrames ---\")\n",
    "    \n",
    "    # Define file paths using the configured directories\n",
    "    csv_path = DATA_DIR_RAW / \"sample_data.csv\"\n",
    "    parquet_path = DATA_DIR_PROCESSED / \"sample_data.parquet\"\n",
    "\n",
    "    # Save the sample DataFrame to CSV\n",
    "    write_df(sample_df, csv_path, 'csv')\n",
    "\n",
    "    # Save the sample DataFrame to Parquet\n",
    "    write_df(sample_df, parquet_path, 'parquet')\n",
    "\n",
    "    print(\"\\n--- Reading DataFrames ---\")\n",
    "\n",
    "    # Read the data back from CSV\n",
    "    df_from_csv = read_df(csv_path)\n",
    "    if df_from_csv is not None:\n",
    "        print(\"\\nDataFrame read from CSV head:\")\n",
    "        print(df_from_csv.head())\n",
    "\n",
    "    # Read the data back from Parquet\n",
    "    df_from_parquet = read_df(parquet_path)\n",
    "    if df_from_parquet is not None:\n",
    "        print(\"\\nDataFrame read from Parquet head:\")\n",
    "        print(df_from_parquet.head())\n",
    "        \n",
    "    # Demonstrate error handling for a missing file\n",
    "    print(\"\\n--- Demonstrating File Not Found Error ---\")\n",
    "    read_df(DATA_DIR_RAW / \"non_existent_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fd614-c9a5-4656-995d-0ba9c95b1cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
