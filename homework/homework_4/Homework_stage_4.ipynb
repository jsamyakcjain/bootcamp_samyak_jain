{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84acdcac-59f6-4d35-ae3b-722aed07c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bootcamp_env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, time, datetime as dt ,csv, pathlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "raw_data =pathlib.Path(\"data/raw\")\n",
    "raw_data.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f332275-e85d-4650-95a8-badab149a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_stamp():\n",
    "    return dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def safe_filename(prefix: str, meta: Dict[str, str]) -> str:\n",
    "    mid = \"_\".join([f\"{k}-{str(v).replace(' ', '-')[:20]}\" for k, v in meta.items()])\n",
    "    return f\"{prefix}_{mid}_{safe_stamp()}.csv\"\n",
    "\n",
    "def validate_df(df: pd.DataFrame, required_cols: List[str], dtypes_map: Dict[str, str]) -> Dict[str, str]:\n",
    "    msgs = {}\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msgs['missing_cols'] = f\"Missing columns: {missing}\"\n",
    "    for col, dtype in dtypes_map.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if dtype == 'datetime64[ns]':\n",
    "                    pd.to_datetime(df[col])\n",
    "                elif dtype == 'float':\n",
    "                    pd.to_numeric(df[col])\n",
    "            except Exception as e:\n",
    "                msgs[f'dtype_{col}'] = f\"Failed to coerce {col} to {dtype}: {e}\"\n",
    "    na_counts = df.isna().sum().sum()\n",
    "    msgs['na_total'] = f\"Total NA values: {na_counts}\"\n",
    "    return msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672e375a-ed1f-4cc5-8334-393a788b507a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ALPHAVANTAGE_API_KEY? True\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"ALPHAVANTAGE_API_KEY\"] = \"2TMJOG8YVAAWX7IV\"\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "print(\"Loaded ALPHAVANTAGE_API_KEY?\",bool(ALPHA_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a1726b-5b0f-4c3c-a138-bf28defc590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching TIME_SERIES_DAILY data for AAPL...\n",
      "             1. open   2. high    3. low  4. close 5. volume\n",
      "2025-08-20  229.9800  230.4700  225.7700  226.0100  42089146\n",
      "2025-08-19  231.2750  232.8700  229.3500  230.5600  39402564\n",
      "2025-08-18  231.7000  233.1200  230.1100  230.8900  37476188\n",
      "2025-08-15  234.0000  234.2800  229.3350  231.5900  56038657\n",
      "2025-08-14  234.0550  235.1200  230.8500  232.7800  51916275\n",
      "...              ...       ...       ...       ...       ...\n",
      "1999-11-05   84.6200   88.3700   84.0000   88.3100   3721500\n",
      "1999-11-04   82.0600   85.3700   80.6200   83.6200   3384700\n",
      "1999-11-03   81.6200   83.2500   81.0000   81.5000   2932700\n",
      "1999-11-02   78.0000   81.6900   77.3100   80.2500   3564600\n",
      "1999-11-01   80.0000   80.6900   77.3700   77.6200   2487300\n",
      "\n",
      "[6490 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. API Pull Function ---\n",
    "def pull_alphavantage_timeseries(\n",
    "    ticker: str,\n",
    "    function: str = 'TIME_SERIES_DAILY',\n",
    "    outputsize: str = 'full',\n",
    "    api_key: str = ALPHA_KEY,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls time series data from Alpha Vantage, handles errors,\n",
    "    and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Alpha Vantage API key not found. Please set it in your .env file.\")\n",
    "\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": function,\n",
    "        \"symbol\": ticker,\n",
    "        \"outputsize\": outputsize,\n",
    "        \"apikey\": api_key,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Fetching {function} data for {ticker}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if 'Error Message' in data:\n",
    "        print(f\"API returned an error: {data['Error Message']}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    time_series_key = \"Time Series (Daily)\"\n",
    "    time_series = data.get(time_series_key)\n",
    "    \n",
    "    if not time_series:\n",
    "        print(\"No time series data found in the response.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    return df\n",
    "    \n",
    "df = pull_alphavantage_timeseries(ticker='AAPL', api_key=ALPHA_KEY)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b5791c-af9a-4a4a-9c52-9cfd4fe9b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
      "\n",
      "--- Raw DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 503 entries, 0 to 502\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Symbol                 503 non-null    object\n",
      " 1   Security               503 non-null    object\n",
      " 2   GICS Sector            503 non-null    object\n",
      " 3   GICS Sub-Industry      503 non-null    object\n",
      " 4   Headquarters Location  503 non-null    object\n",
      " 5   Date added             503 non-null    object\n",
      " 6   CIK                    503 non-null    object\n",
      " 7   Founded                503 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 31.6+ KB\n",
      "None\n",
      "\n",
      "First 5 rows of raw DataFrame:\n",
      "  Symbol             Security             GICS Sector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
      "\n",
      "          CIK      Founded  \n",
      "0  0000066740         1902  \n",
      "1  0000091142         1916  \n",
      "2  0000001800         1888  \n",
      "3  0001551152  2013 (1888)  \n",
      "4  0001467373         1989  \n",
      "\n",
      "--- Validation Report ---\n",
      "- na_total: Total NA values: 0\n",
      "\n",
      "--- Final DataFrame Summary ---\n",
      "Shape: (503, 8)\n",
      "Data Types:\n",
      " Symbol                           object\n",
      "Security                         object\n",
      "GICS Sector                      object\n",
      "GICS Sub-Industry                object\n",
      "Headquarters Location            object\n",
      "Date added               datetime64[ns]\n",
      "CIK                              object\n",
      "Founded                          object\n",
      "dtype: object\n",
      "\n",
      "Head of Final DataFrame:\n",
      "  Symbol             Security             GICS Sector  \\\n",
      "0    MMM                   3M             Industrials   \n",
      "1    AOS          A. O. Smith             Industrials   \n",
      "2    ABT  Abbott Laboratories             Health Care   \n",
      "3   ABBV               AbbVie             Health Care   \n",
      "4    ACN            Accenture  Information Technology   \n",
      "\n",
      "                GICS Sub-Industry    Headquarters Location Date added  \\\n",
      "0        Industrial Conglomerates    Saint Paul, Minnesota 1957-03-04   \n",
      "1               Building Products     Milwaukee, Wisconsin 2017-07-26   \n",
      "2           Health Care Equipment  North Chicago, Illinois 1957-03-04   \n",
      "3                   Biotechnology  North Chicago, Illinois 2012-12-31   \n",
      "4  IT Consulting & Other Services          Dublin, Ireland 2011-07-06   \n",
      "\n",
      "          CIK      Founded  \n",
      "0  0000066740         1902  \n",
      "1  0000091142         1916  \n",
      "2  0000001800         1888  \n",
      "3  0001551152  2013 (1888)  \n",
      "4  0001467373         1989  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_RAW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# --- Save Raw Data ---\u001b[39;00m\n\u001b[1;32m     87\u001b[0m meta_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikipedia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp500-components\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 88\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[43mDATA_RAW\u001b[49m \u001b[38;5;241m/\u001b[39m safe_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweb_scrape\u001b[39m\u001b[38;5;124m'\u001b[39m, meta_info)\n\u001b[1;32m     89\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSuccessfully saved raw data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_RAW' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the URL for the S&P 500 Wikipedia page\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    \n",
    "    print(f\"Fetching data from: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Request the HTML content of the page\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the main table containing the S&P 500 components\n",
    "        # The table has the CSS class 'wikitable' and 'sortable'\n",
    "        sp500_table = soup.find('table', {'class': 'wikitable'})\n",
    "        \n",
    "        if not sp500_table:\n",
    "            raise ValueError(\"Could not find the S&P 500 table on the page.\")\n",
    "        \n",
    "        # Extract headers from the table\n",
    "        headers = [th.text.strip() for th in sp500_table.find_all('th')]\n",
    "        \n",
    "        # Extract data rows\n",
    "        data_rows = []\n",
    "        for row in sp500_table.find_all('tr')[1:]:  # Skip the header row\n",
    "            cols = row.find_all('td')\n",
    "            # Extract text from each cell and strip whitespace\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            data_rows.append(cols)\n",
    "            \n",
    "        # Create a pandas DataFrame from the extracted data and headers\n",
    "        df = pd.DataFrame(data_rows, columns=headers)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        exit()\n",
    "    except ValueError as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        exit()\n",
    "        \n",
    "    # --- Data Cleaning and Validation ---\n",
    "    print(\"\\n--- Raw DataFrame Info ---\")\n",
    "    print(df.info())\n",
    "    print(\"\\nFirst 5 rows of raw DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Drop the 'SEC filings' column as it's not needed for the core data\n",
    "    if 'SEC filings' in df.columns:\n",
    "        df = df.drop(columns=['SEC filings'])\n",
    "    \n",
    "    # Define the columns we want to keep and their data types\n",
    "    required_cols = ['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Headquarters Location', 'Date added']\n",
    "    dtype_map = {\n",
    "        'Symbol': 'string',\n",
    "        'Security': 'string',\n",
    "        'GICS Sector': 'string',\n",
    "        'GICS Sub-Industry': 'string',\n",
    "        'Headquarters Location': 'string',\n",
    "        'Date added': 'datetime64[ns]'\n",
    "    }\n",
    "    \n",
    "    # Check for presence of required columns\n",
    "    if not set(required_cols).issubset(df.columns):\n",
    "        print(f\"\\nMissing required columns. Found: {df.columns.tolist()}\")\n",
    "        exit()\n",
    "        \n",
    "    # Apply the data types and perform validation\n",
    "    df['Date added'] = pd.to_datetime(df['Date added'], errors='coerce')\n",
    "    validation_msgs = validate_df(df, required_cols, dtype_map)\n",
    "    \n",
    "    print(\"\\n--- Validation Report ---\")\n",
    "    if validation_msgs:\n",
    "        for key, msg in validation_msgs.items():\n",
    "            print(f\"- {key}: {msg}\")\n",
    "    else:\n",
    "        print(\"âœ… Data validation successful. No issues found.\")\n",
    "\n",
    "    print(\"\\n--- Final DataFrame Summary ---\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Data Types:\\n\", df.dtypes)\n",
    "    print(\"\\nHead of Final DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # --- Save Raw Data ---\n",
    "    meta_info = {'source': 'wikipedia', 'content': 'sp500-components'}\n",
    "    file_path = DATA_RAW / safe_filename('web_scrape', meta_info)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved raw data to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ba6a5-ca31-456b-b867-95276a9524aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
